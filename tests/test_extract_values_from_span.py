import pytest
import spacy
from wikimeasurements.utils.nlp_utils import extract_values_from_span, inline_tags_to_char_offsets
from wikimeasurements.utils.quantity_utils import get_basic_number_words


def test_extract_values_from_span():
    """Test if extract_values_from_span correctly identifies numbers in text."""

    test_strings = [
        ["It covers an area of ğŸ191,123ğŸ sq mi, with a population of ğŸ15.9 millionğŸ as of 2021."],
        ["It is ğŸ9.1ğŸ km long and ğŸ9.3ğŸ km wide."],
        ["The Forschungszentrum JÃ¼lich is a member of the Helmholtz Association."],
        ["The building costs $ğŸ32,241,700ğŸ to build including the demolition of the roof part."],
        ["The geographical mile is determined by ğŸ1ğŸ minute of arc along the Earth's equator."],        
        ["Gallium has the atomic number ğŸ31ğŸ."],        
        ["The national census of 2020 recorded its population as approximately ğŸ1,213,456ğŸ."],
        ["The total area is generally stated as being ğŸ1,988,213ğŸ sq mi."],
        ["Earth's mass is around ğŸ5,900ğŸ Yg."],
        ["The Earth is an ellipsoid and has a circumference of ~ ğŸ40,000ğŸ km."],
        ["The highest temperature ever recorded was ğŸ39.2ğŸ Â°C on April 01, 1992."],
        ["Until 1964, the bridge was the longest suspension bridge with ğŸ4200ğŸ feet and ğŸ67ğŸ m clearance above high water."],
        ["It is an alga which may grow up in length to ğŸ12ğŸ metres (ğŸ39ğŸ ft)."],
        ["Numbers can be written in different ways, e.g., ğŸ270ğŸ can be written as ğŸ2.7Ã—102ğŸ or ğŸ27Ã—101ğŸ or ğŸ270Ã—100ğŸ."],
        ["ğŸ350ğŸ can be written as ğŸ3.5Ã—10^2ğŸ or ğŸ35Ã—10^1ğŸ or ğŸ350Ã—10^0ğŸ."],
        ["ğŸ350ğŸ can be written as ğŸ3.5 10Â²ğŸ or ğŸ35 10^1ğŸ or ğŸ350 10^0ğŸ."],
        ["ğŸ350ğŸ can be written as ğŸ3.5*10^2ğŸ or ğŸ35*10^1ğŸ or ğŸ350*10^0ğŸ."],
        ["ğŸ3500ğŸ can be written as ğŸ35.0Ã—10^2ğŸ or ğŸ350Ã—10^1ğŸ or ğŸ3,500Ã—10^0ğŸ."],
        ["ğŸOneğŸ, ğŸtwoğŸ and ğŸthreeğŸ"],
    ]

    nlp = spacy.load("en_core_web_md")
    nlp.add_pipe("merge_entities")
    nlp.add_pipe("custom_sentencizer", before="parser")
    nlp.add_pipe("lower_lemmas")

    with nlp.select_pipes(disable=["merge_entities", "ner"]):
        WIKI_ORDINALS_LOWER = get_basic_number_words(
            numeral_type="ordinals_minus_denominator_intersection"
        )
        WIKI_ORDINAL_LEMMAS = get_basic_number_words(numeral_type="ordinals")
        WIKI_ORDINAL_LEMMAS = list(nlp.pipe(WIKI_ORDINAL_LEMMAS))
        WIKI_ORDINAL_LEMMAS = list(set([tokens[0].lemma for tokens in WIKI_ORDINAL_LEMMAS]))

    for texts in test_strings:
        
        all_texts = []
        num_offsets_solution = []
        for sentence in texts:
            clean_text, num_offset = inline_tags_to_char_offsets(sentence)
            all_texts.append(clean_text)
            num_offsets_solution.append(num_offset["value"])
        
        full_text = " ".join(all_texts)
        
        with nlp.select_pipes(disable=["merge_entities"]):            
            doc = nlp(full_text)

        num_offsets = []
        for sent in doc.sents:
            num_offsets.append(extract_values_from_span(
                sent,
                doc,
                consider_ordinals=False,
                WIKI_ORDINAL_LEMMAS=WIKI_ORDINAL_LEMMAS,
                WIKI_ORDINALS_LOWER=WIKI_ORDINALS_LOWER,
            ))

        assert num_offsets == num_offsets_solution


if __name__ == "__main__":
    test_extract_values_from_span()
